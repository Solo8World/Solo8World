<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="个人不完全技术博客"><title>保姆级教程：如何本地部署私有chatglm模型及知识库扩展调试 | Lee Blog</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=1.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/latest/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/latest/pure-min.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/latest/grids-responsive-min.min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/latest/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"><script type="text/javascript" src="//lib.baomitu.com/clipboard.js/latest/clipboard.min.js"></script><script type="text/javascript" src="//lib.baomitu.com/toastr.js/latest/toastr.min.js"></script><link rel="stylesheet" href="//lib.baomitu.com/toastr.js/latest/toastr.min.css"><div class="darkmode-toggle">🌓</div><script>var prefersDarkMode = window.matchMedia('(prefers-color-scheme: dark)');
var toggle = document.querySelector('.darkmode-toggle');
var html = document.querySelector('html');

html.dataset.dark = localStorage.dark || prefersDarkMode.matches;

toggle.addEventListener('click', () => {
localStorage.dark = !(html.dataset.dark == 'true');
html.dataset.dark = localStorage.dark;
});</script><meta name="generator" content="Hexo 5.1.1"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">保姆级教程：如何本地部署私有chatglm模型及知识库扩展调试</h1><a id="logo" href="/.">Lee Blog</a><p class="description">Less is more.  enmm... except hair</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">保姆级教程：如何本地部署私有chatglm模型及知识库扩展调试</h1><div class="post-meta">2023-06-30<span> | </span><span class="category"><a href="/categories/AIGC/">AIGC</a></span><script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> 阅读</span></span></div><div class="clear"><div class="toc-article" id="toc"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%83%A8%E7%BD%B2%E6%9D%A1%E4%BB%B6%E5%87%86%E5%A4%87"><span class="toc-number">1.</span> <span class="toc-text">部署条件准备</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BC%80%E5%A7%8B%E9%83%A8%E7%BD%B2"><span class="toc-number">2.</span> <span class="toc-text">开始部署</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%88%91%E7%9A%84%E9%83%A8%E7%BD%B2%E7%8E%AF%E5%A2%83"><span class="toc-number">2.1.</span> <span class="toc-text">我的部署环境</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%83%A8%E7%BD%B2%E6%AD%A5%E9%AA%A4"><span class="toc-number">2.2.</span> <span class="toc-text">部署步骤</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A3%80%E6%9F%A5%E4%BD%A0%E7%9A%84python%E7%89%88%E6%9C%AC"><span class="toc-number">2.2.1.</span> <span class="toc-text">检查你的python版本</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%8B%E8%BD%BDgitchain-chatglm%E4%BB%A3%E7%A0%81"><span class="toc-number">2.2.2.</span> <span class="toc-text">下载gitchain-chatglm代码</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%89%E8%A3%85%E4%BE%9D%E8%B5%96"><span class="toc-number">2.2.3.</span> <span class="toc-text">安装依赖</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%8B%E8%BD%BD%E6%A8%A1%E5%9E%8B"><span class="toc-number">2.2.4.</span> <span class="toc-text">下载模型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%BF%90%E8%A1%8C"><span class="toc-number">2.2.5.</span> <span class="toc-text">运行</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%8A%A5%E9%94%99%EF%BC%9ARuntimeError-Library-cudart-is-not-initialized"><span class="toc-number">2.2.5.1.</span> <span class="toc-text">报错：RuntimeError: Library cudart is not initialized</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%85%B6%E4%BB%96%E6%8A%A5%E9%94%99"><span class="toc-number">2.2.6.</span> <span class="toc-text">其他报错</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#conda%E7%9A%84%E4%BD%BF%E7%94%A8"><span class="toc-number">3.</span> <span class="toc-text">conda的使用</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%89%E8%A3%85conda"><span class="toc-number">3.1.</span> <span class="toc-text">安装conda</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA%E6%96%B0%E7%8E%AF%E5%A2%83"><span class="toc-number">3.2.</span> <span class="toc-text">创建新环境</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%BF%80%E6%B4%BB%E7%8E%AF%E5%A2%83"><span class="toc-number">3.3.</span> <span class="toc-text">激活环境</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%80%80%E5%87%BA%E7%8E%AF%E5%A2%83"><span class="toc-number">3.4.</span> <span class="toc-text">退出环境</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%A0%E9%99%A4%E7%8E%AF%E5%A2%83"><span class="toc-number">3.5.</span> <span class="toc-text">删除环境</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9F%A5%E7%9C%8B%E5%B7%B2%E6%9C%89%E7%8E%AF%E5%A2%83"><span class="toc-number">3.6.</span> <span class="toc-text">查看已有环境</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%89%E8%A3%85python%E5%BA%93%EF%BC%88%E7%A4%BA%E4%BE%8B%EF%BC%8C%E9%9D%9E%E5%BF%85%E8%A6%81%EF%BC%89"><span class="toc-number">3.7.</span> <span class="toc-text">安装python库（示例，非必要）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#zip%E5%8E%8B%E7%BC%A9%E4%B8%8E%E8%A7%A3%E5%8E%8B%E7%BC%A9"><span class="toc-number">4.</span> <span class="toc-text">zip压缩与解压缩</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8E%8B%E7%BC%A9"><span class="toc-number">4.1.</span> <span class="toc-text">压缩</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A7%A3%E5%8E%8B%E7%BC%A9"><span class="toc-number">4.2.</span> <span class="toc-text">解压缩</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#rsync-%E7%9A%84%E4%BD%BF%E7%94%A8"><span class="toc-number">5.</span> <span class="toc-text">rsync 的使用</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%89%E8%A3%85"><span class="toc-number">5.1.</span> <span class="toc-text">安装</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8"><span class="toc-number">5.2.</span> <span class="toc-text">使用</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%89%A9%E5%B1%95%E9%98%85%E8%AF%BB"><span class="toc-number">6.</span> <span class="toc-text">扩展阅读</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A3%B0%E6%98%8E"><span class="toc-number">7.</span> <span class="toc-text">声明</span></a></li></ol></div></div><div class="post-content"><h2 id="部署条件准备"><a href="#部署条件准备" class="headerlink" title="部署条件准备"></a>部署条件准备</h2><ul>
<li>一台有显卡的linux服务器，显存达到8G以上，内存达到8G以上，硬盘空间达到100G以上</li>
</ul>
<h2 id="开始部署"><a href="#开始部署" class="headerlink" title="开始部署"></a>开始部署</h2><h3 id="我的部署环境"><a href="#我的部署环境" class="headerlink" title="我的部署环境"></a>我的部署环境</h3><ul>
<li>系统：Ubuntu 16.04</li>
<li>显卡：NVIDIA GeForce GTX 2080 Ti</li>
<li>显存：11 G</li>
<li>内存：64 G</li>
<li>硬盘：1T</li>
</ul>
<h3 id="部署步骤"><a href="#部署步骤" class="headerlink" title="部署步骤"></a>部署步骤</h3><h4 id="检查你的python版本"><a href="#检查你的python版本" class="headerlink" title="检查你的python版本"></a>检查你的python版本</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">python3 --version</span><br></pre></td></tr></table></figure>
<p>建议python3.8 - 3.10最佳，如果不是，建议安装conda，创建一个新的python环境，详细见下文conda的安装使用</p>
<h4 id="下载gitchain-chatglm代码"><a href="#下载gitchain-chatglm代码" class="headerlink" title="下载gitchain-chatglm代码"></a>下载gitchain-chatglm代码</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/imClumsyPanda/langchain-ChatGLM.git</span><br></pre></td></tr></table></figure>

<h4 id="安装依赖"><a href="#安装依赖" class="headerlink" title="安装依赖"></a>安装依赖</h4><p>需要提前安装paddleocr依赖libX11，libXext</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">apt install libx11-dev libxext-dev libxtst-dev libxrender-dev libxmu-dev libxmuu-dev </span><br><span class="line">apt install libXext</span><br></pre></td></tr></table></figure>

<p>然后安装python依赖：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> langchain-ChatGLM</span><br><span class="line">pip install -r requirements.txt</span><br></pre></td></tr></table></figure>
<p>检查有无问题，看不懂的话，就不要管了，反正我也看不懂。<br>实在要管的话，就复制报错信息，发给chatGPT询问。</p>
<h4 id="下载模型"><a href="#下载模型" class="headerlink" title="下载模型"></a>下载模型</h4><p>该项目使用了两种模型，一种是在上传知识库文件时，用来语义分段和向量转化的embedding模型,如text2vec，<br>一种是用来生成回复的LLM模型，如chatGLM。</p>
<p>如果你网络环境好，人在美国刚下飞机，可以不用手动下载，直接运行python webui.py，会自动下载模型<br>如果不好（大概率不好），请手动下载模型，注意这里未必需要全部下载，embedding模型和llm模型各下载一个就可以，<br>手动下载模型有两个方法：</p>
<p>1.使用百度网盘下载(我这边蹭的同事的会员，大概20分钟下载完毕)</p>
<p>  a. ernie-3.0-base-zh.zip <a target="_blank" rel="noopener" href="https://pan.baidu.com/s/1CIvKnD3qzE-orFouA8qvNQ?pwd=4wih">链接</a></p>
<p>  b. ernie-3.0-nano-zh.zip <a target="_blank" rel="noopener" href="https://pan.baidu.com/s/1Fh8fgzVdavf5P1omAJJ-Zw?pwd=q6s5">链接</a></p>
<p>  c. text2vec-large-chinese.zip <a target="_blank" rel="noopener" href="https://pan.baidu.com/s/1sMyPzBIXdEzHygftEoyBuA?pwd=4xs7">链接</a></p>
<p>  d. chatglm-6b-int4-qe.zip <a target="_blank" rel="noopener" href="https://pan.baidu.com/s/1DDKMOMHtNZccOOBGWIOYww?pwd=22ji">链接</a></p>
<p>  e. chatglm-6b-int4.zip <a target="_blank" rel="noopener" href="https://pan.baidu.com/s/1pvZ6pMzovjhkA6uPcRLuJA?pwd=3gjd">链接</a></p>
<p>  f. chatglm-6b.zip <a target="_blank" rel="noopener" href="https://pan.baidu.com/s/1B-MpsVVs1GHhteVBetaquw?pwd=djay">链接</a></p>
<p>2.访问huggingface网页，找到相关模型项目，自己在电脑上建个文件夹，<br>然后在hugginnface的file目录里挨个点击下载（有时候huggingface的访问会不通，多试几次）</p>
<p>huggingface官网：<a target="_blank" rel="noopener" href="https://huggingface.co/">https://huggingface.co/</a></p>
<p>llm模型（chatglm-6b-int4）：<a target="_blank" rel="noopener" href="https://huggingface.co/THUDM/chatglm-6b-int4/tree/main">https://huggingface.co/THUDM/chatglm-6b-int4/tree/main</a></p>
<p>embedding模型（text2vec-large-chinese）：<a target="_blank" rel="noopener" href="https://huggingface.co/THUDM/text2vec-large-chinese/tree/main">https://huggingface.co/THUDM/text2vec-large-chinese/tree/main</a></p>
<p>这个方法适用于chatglm2 和一些其他后期你想扩展使用的模型</p>
<p>下载完成后，上传并解压到服务器任一目录，然后修改config.py里的模型路径</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> langchain-ChatGLM</span><br><span class="line">vim config/model_config.py</span><br></pre></td></tr></table></figure>

<p>在config文件里需要修改两处</p>
<p>1.修改embedding模型路径，我的路径如下”/root/langchain/embedding/“，你的路径可能不同，需要修改</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 在以下字典中修改属性值，以指定本地embedding模型存储位置</span></span><br><span class="line"><span class="comment"># 如将 &quot;text2vec&quot;: &quot;GanymedeNil/text2vec-large-chinese&quot; 修改为 &quot;text2vec&quot;: &quot;User/Downloads/text2vec-large-chinese&quot;</span></span><br><span class="line"><span class="comment"># 此处请写绝对路径</span></span><br><span class="line">embedding_model_dict = &#123;</span><br><span class="line">    <span class="string">&quot;ernie-tiny&quot;</span>: <span class="string">&quot;/root/langchain/embedding/ernie-3.0-nano-zh&quot;</span>,</span><br><span class="line">    <span class="string">&quot;ernie-base&quot;</span>: <span class="string">&quot;nghuyong/ernie-3.0-base-zh&quot;</span>,</span><br><span class="line">    <span class="string">&quot;text2vec-base&quot;</span>: <span class="string">&quot;shibing624/text2vec-base-chinese&quot;</span>,</span><br><span class="line">    <span class="string">&quot;text2vec&quot;</span>: <span class="string">&quot;/root/langchain/embedding/text2vec-large-chinese&quot;</span>,</span><br><span class="line">    <span class="string">&quot;m3e-small&quot;</span>: <span class="string">&quot;/root/langchain/embedding/m3e-small&quot;</span>,</span><br><span class="line">    <span class="string">&quot;m3e-base&quot;</span>: <span class="string">&quot;/root/langchain/embedding/m3e-base&quot;</span>,</span><br><span class="line">&#125;       </span><br></pre></td></tr></table></figure>
<p>2.修改LLM模型路径，我的路径如下”/root/langchain/llm/“，你的路径可能不同，需要修改</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># llm_model_dict 处理了loader的一些预设行为，如加载位置，模型名称，模型处理器实例</span></span><br><span class="line"><span class="comment"># 在以下字典中修改属性值，以指定本地 LLM 模型存储位置</span></span><br><span class="line"><span class="comment"># 如将 &quot;chatglm-6b&quot; 的 &quot;local_model_path&quot; 由 None 修改为 &quot;User/Downloads/chatglm-6b&quot;</span></span><br><span class="line"><span class="comment"># 此处请写绝对路径</span></span><br><span class="line">llm_model_dict = &#123;</span><br><span class="line">    <span class="string">&quot;chatglm-6b-int4-qe&quot;</span>: &#123;</span><br><span class="line">        <span class="string">&quot;name&quot;</span>: <span class="string">&quot;chatglm-6b-int4-qe&quot;</span>,</span><br><span class="line">        <span class="string">&quot;pretrained_model_name&quot;</span>: <span class="string">&quot;chatglm-6b-int4-qe&quot;</span>,</span><br><span class="line">        <span class="string">&quot;local_model_path&quot;</span>: <span class="string">&quot;/root/langchain/llm/chatglm-6b-int4-qe&quot;</span>,</span><br><span class="line">        <span class="string">&quot;provides&quot;</span>: <span class="string">&quot;ChatGLM&quot;</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">&quot;chatglm-6b-int4&quot;</span>: &#123;</span><br><span class="line">        <span class="string">&quot;name&quot;</span>: <span class="string">&quot;chatglm-6b-int4&quot;</span>,</span><br><span class="line">        <span class="string">&quot;pretrained_model_name&quot;</span>: <span class="string">&quot;chatglm-6b-int4&quot;</span>,</span><br><span class="line">        <span class="string">&quot;local_model_path&quot;</span>: <span class="string">&quot;/root/langchain/llm/chatglm-6b-int4&quot;</span>,</span><br><span class="line">        <span class="string">&quot;provides&quot;</span>: <span class="string">&quot;ChatGLM&quot;</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">&quot;chatglm2-6b-int4&quot;</span>: &#123;</span><br><span class="line">         <span class="string">&quot;name&quot;</span>: <span class="string">&quot;chatglm2-6b-int4&quot;</span>,</span><br><span class="line">         <span class="string">&quot;pretrained_model_name&quot;</span>: <span class="string">&quot;chatglm2-6b-int4&quot;</span>,</span><br><span class="line">         <span class="string">&quot;local_model_path&quot;</span>: <span class="string">&quot;/root/langchain/llm/chatglm2-6b-int4&quot;</span>,</span><br><span class="line">         <span class="string">&quot;provides&quot;</span>: <span class="string">&quot;ChatGLM&quot;</span></span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>3.修改webui.py启动时默认加载的模型，同样在config/model_config.py里修改</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Embedding model name</span></span><br><span class="line">EMBEDDING_MODEL = <span class="string">&quot;m3e-base&quot;</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># LLM 名称</span></span><br><span class="line">LLM_MODEL = <span class="string">&quot;chatglm2-6b-int4&quot;</span></span><br></pre></td></tr></table></figure>


<h4 id="运行"><a href="#运行" class="headerlink" title="运行"></a>运行</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">python webui.py</span><br></pre></td></tr></table></figure>
<p>如果没有报错，就可以打开浏览器，输入http://服务器ip:7860/，就可以看到webui的调试界面了</p>
<p>如果报错（大概率报错）<br>看你报的什么错咯，以下是我遇到的问题及解决方案</p>
<h5 id="报错：RuntimeError-Library-cudart-is-not-initialized"><a href="#报错：RuntimeError-Library-cudart-is-not-initialized" class="headerlink" title="报错：RuntimeError: Library cudart is not initialized"></a>报错：RuntimeError: Library cudart is not initialized</h5><p>这个错误在webui.py启动时并不会直接出现，只是提示模型加载失败。<br>所以建议直接 python cli-demo.py，更容易暴露出错误信息</p>
<p>排查步骤：</p>
<p>1.执行init看torch是否正常初始化</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">python -c <span class="string">&quot;import torch;torch.cuda.init();print(torch.cuda.is_available())&quot;</span></span><br></pre></td></tr></table></figure>

<p>2.检查cuda版本</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">python -c <span class="string">&quot;import torch; print(torch.version.cuda)&quot;</span></span><br></pre></td></tr></table></figure>


<p>4.如果你的版本低于11.7(chatGLM要求的最低版本)，请升级cuda：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">conda install cudatoolkit=11.7 -c nvidia</span><br></pre></td></tr></table></figure>

<h4 id="其他报错"><a href="#其他报错" class="headerlink" title="其他报错"></a>其他报错</h4><p>其他报错我就没遇到了，如果你遇到了，优先询问chatGPT，<br>次之自己按以下文档顺序查找相关信息(按顺序！)，最后加群或者在这里留言解答</p>
<ul>
<li>langchain-ChatGLM常见问题： <a target="_blank" rel="noopener" href="https://github.com/imClumsyPanda/langchain-ChatGLM/blob/master/docs/FAQ.md">https://github.com/imClumsyPanda/langchain-ChatGLM/blob/master/docs/FAQ.md</a></li>
<li>安装文档（网友提供，里面的模型修改部分跟最新版本有出入，注意分辨）：<a target="_blank" rel="noopener" href="https://d29l201m55.feishu.cn/docx/LFREdCeOIoIdSQxpCG7cDpAJnJe">https://d29l201m55.feishu.cn/docx/LFREdCeOIoIdSQxpCG7cDpAJnJe</a></li>
<li>langchain-ChatGLM的issues区：<a target="_blank" rel="noopener" href="https://github.com/imClumsyPanda/langchain-ChatGLM/issues">https://github.com/imClumsyPanda/langchain-ChatGLM/issues</a></li>
<li>ChatGLM的issues区(有时候确实是模型加载的问题，可在这上面找)：<a target="_blank" rel="noopener" href="https://github.com/THUDM/ChatGLM-6B/issues">https://github.com/THUDM/ChatGLM-6B/issues</a></li>
</ul>
<h2 id="conda的使用"><a href="#conda的使用" class="headerlink" title="conda的使用"></a>conda的使用</h2><p>conda可以帮我们快速的创建/切换python环境，<br>方便我们在不同的项目中使用不同的python版本，不同的python库版本，避免版本冲突的问题。</p>
<p>conda分为anaconda 和 miniconda，anaconda 是一个包含了许多常用库的集合版本，miniconda 是精简版本（只包含conda、pip、zlib、python 以及它们所需的包）.<br>我这里使用的是miniconda，因为我不需要anaconda中的那些库，而且anaconda的安装包比较大，下载起来比较慢。</p>
<h3 id="安装conda"><a href="#安装conda" class="headerlink" title="安装conda"></a>安装conda</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh</span><br><span class="line">bash Miniconda3-latest-Linux-x86_64.sh</span><br></pre></td></tr></table></figure>
<p>安装过程中会提示你是否将conda加入到环境变量中，选择yes即可。</p>
<h3 id="创建新环境"><a href="#创建新环境" class="headerlink" title="创建新环境"></a>创建新环境</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">conda create -n chatglm python=3.8</span><br></pre></td></tr></table></figure>
<h3 id="激活环境"><a href="#激活环境" class="headerlink" title="激活环境"></a>激活环境</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">conda activate chatglm</span><br></pre></td></tr></table></figure>
<h3 id="退出环境"><a href="#退出环境" class="headerlink" title="退出环境"></a>退出环境</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">conda deactivate</span><br></pre></td></tr></table></figure>
<h3 id="删除环境"><a href="#删除环境" class="headerlink" title="删除环境"></a>删除环境</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">conda remove -n chatglm --all</span><br></pre></td></tr></table></figure>
<h3 id="查看已有环境"><a href="#查看已有环境" class="headerlink" title="查看已有环境"></a>查看已有环境</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">conda info --envs</span><br></pre></td></tr></table></figure>
<h3 id="安装python库（示例，非必要）"><a href="#安装python库（示例，非必要）" class="headerlink" title="安装python库（示例，非必要）"></a>安装python库（示例，非必要）</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">conda install -n chatglm numpy</span><br></pre></td></tr></table></figure>


<h2 id="zip压缩与解压缩"><a href="#zip压缩与解压缩" class="headerlink" title="zip压缩与解压缩"></a>zip压缩与解压缩</h2><h3 id="压缩"><a href="#压缩" class="headerlink" title="压缩"></a>压缩</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">zip -r chatglm.zip chatglm</span><br></pre></td></tr></table></figure>
<h3 id="解压缩"><a href="#解压缩" class="headerlink" title="解压缩"></a>解压缩</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">unzip chatglm.zip</span><br></pre></td></tr></table></figure>

<h2 id="rsync-的使用"><a href="#rsync-的使用" class="headerlink" title="rsync 的使用"></a>rsync 的使用</h2><p>rsync是一个远程数据同步工具，可通过LAN/WAN快速同步多台主机间的文件。rsync使用所谓的“rsync算法”来使本地和远程两个主机之间的文件达到同步，<br>这个算法只传送两个文件的不同部分，而不是每次都整份传送，因此速度保证。<br>我平时小文件传输一般直接scp即可，但是如果文件比较大，或者需要频繁传输，就需要用rsync了。scp是加密传输，所以相对安全，但效率也低。往往出现越传越慢的情况。<br>所以这里用了rsync，速度快，效率高，但是不加密，所以不要传输敏感文件。</p>
<h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt install rsync</span><br></pre></td></tr></table></figure>
<h3 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rsync -av --info=progress2  chatglm-6b-int4-qe.zip root@服务器ip:/root/langchain/llm/</span><br></pre></td></tr></table></figure>
<p>其中，-av –info=progress2  是参数，chatglm-6b-int4-qe.zip 是要传输的文件，root@服务器ip:/root/langchain/llm/ 是目标地址。<br>-av –info=progress2参数表示：-a是归档模式，-v是显示详细信息，–info=progress2是显示传输进度。</p>
<h2 id="扩展阅读"><a href="#扩展阅读" class="headerlink" title="扩展阅读"></a>扩展阅读</h2><ul>
<li><a target="_blank" rel="noopener" href="https://github.com/imClumsyPanda/langchain-ChatGLM">langchain-ChatGLM项目地址</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/THUDM/ChatGLM-6B">ChatGLM-6b项目地址</a></li>
<li><a target="_blank" rel="noopener" href="https://python.langchain.com/">langchain框架文档</a></li>
<li><a target="_blank" rel="noopener" href="https://www.heywhale.com/mw/project/643977aa446c45f4592a1e59">ModelWhale 平台实现</a></li>
<li><a target="_blank" rel="noopener" href="https://cloud.tencent.com/developer/article/2063866">cuda与显卡驱动的适配问题</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/thomas-yanxin/LangChain-ChatGLM-Webui/blob/master/docs/deploy.md">另一个基于langchat-ChatGLM的ui项目</a></li>
</ul>
<h2 id="声明"><a href="#声明" class="headerlink" title="声明"></a>声明</h2><p><code>本文60%的内容由github-copilot辅助提示生成，本人部署过程中50%的问题由chatGPT解答并解决。</code></p>
<p><strong>本文作者</strong>：Lee<br/><strong>本文地址</strong>： <a href="leeblog.icu/2023/06/30/">leeblog.icu/2023/06/30/</a> <br/><strong>版权声明</strong>：本博客所有文章除特别声明外，均采用 <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC 4.0 BY-NC-SA</a> 许可协议。转载请注明出处！</p>
</div><div id="donate"><link rel="stylesheet" type="text/css" href="/css/donate.css?v=1.0.0"><script type="text/javascript" src="/js/donate.js?v=1.0.0" successtext="复制成功!"></script><a class="pos-f tr3" id="github" href="https://github.com/Kaiyuan/donate-page" target="_blank" title="Github"></a><div id="DonateText">Donate</div><ul class="list pos-f" id="donateBox"><li id="AliPay" qr="/alipay.jpeg"></li><li id="WeChat" qr="/wechat.jpeg"></li></ul><div class="pos-f left-100" id="QRBox"><div id="MainBox"></div></div></div><script type="text/javascript" src="/js/share.js?v=1.0.0" async></script><a class="article-share-link" data-url="leeblog.icu/2023/06/30/" data-id="cljlbt8mz000wzyxbelu8c5j1" data-qrcode="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAJYAAACWCAAAAAAZai4+AAABUklEQVR42u3aMRLDIAxEUd//0k6bIpBdJArYT+XxxOY5M4BAeh65vXIb/f7Z0WDBuojldqPf+b6v9wULVjJrNELn1/POlGeH92HBgiWzlEUaFixYO1g6HRYsWC6rMrDnG9TtsTwsWFew9FOd+vWW8y1YsA5nvWZbG/Z2L7BghbH0AawfD80/QH8/LFg5LDdVqYS5a0lQWLBgKVNA5ZhJScZI+11YsC5lVcJfZSFfK1CABSuN5XajP6tMK05gDwvWzawKWklnuiULDf8lLFjHsowigEIaxn4nLFiRLHcwdy3bRugMC1YYy51QuqoefizVsGAFsCoFB27pjzINwYKVyVo7vTESlr1hNCxYV7Mqr3ZDaiOxCgtWJKuyDDenT2DBgmVuSvWEqL3RhQULVtPxkL4l/lMhCAtWDEt5rGuasJMrsGAFsOplBGvp0ubAGhasU1kfZUkNNM5XEBcAAAAASUVORK5CYII=">分享</a><div class="tags"><a href="/tags/技术"><i class="fa fa-tag">技术</i></a></div><div class="post-nav"><a class="next" href="/2023/06/24/">LangChain+ChatGLM模型自建及评测</a></div><div id="lv-container" data-id="city" data-uid="MTAyMC81MTcwMC8yODE4MQ=="><script>(function(d, s) {
  var j, e = d.getElementsByTagName(s)[0];
  if (typeof LivereTower === 'function') { return; }
  j = d.createElement(s);
  j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
  j.async = true;
  e.parentNode.insertBefore(j, e);
})(document, 'script');
</script></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><div class="search-form"><input id="local-search-input" placeholder="Search" type="text" name="q" results="0"/><div id="local-search-result"></div></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/AIGC/">AIGC</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%8A%80%E6%9C%AF/">技术</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%8A%80%E6%9C%AF/linux/">linux</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%8A%80%E6%9C%AF/%E4%BB%A3%E7%A0%81/">代码</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%8A%80%E6%9C%AF/%E5%B7%A5%E5%85%B7/">工具</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E9%9A%8F%E7%AC%94/">随笔</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/GIT/" style="font-size: 15px;">GIT</a> <a href="/tags/%E5%BC%80%E5%8F%91%E8%A7%84%E8%8C%83/" style="font-size: 15px;">开发规范</a> <a href="/tags/%E5%B7%A5%E5%85%B7/" style="font-size: 15px;">工具</a> <a href="/tags/%E6%8F%92%E4%BB%B6/" style="font-size: 15px;">插件</a> <a href="/tags/%E9%9A%8F%E7%AC%94/" style="font-size: 15px;">随笔</a> <a href="/tags/java/" style="font-size: 15px;">java</a> <a href="/tags/x%E9%A1%B5%E4%BB%A3%E7%A0%81/" style="font-size: 15px;">x页代码</a> <a href="/tags/wechat/" style="font-size: 15px;">wechat</a> <a href="/tags/%E4%BB%A3%E7%A0%81/" style="font-size: 15px;">代码</a> <a href="/tags/%E6%96%87%E6%A1%A3/" style="font-size: 15px;">文档</a> <a href="/tags/arch/" style="font-size: 15px;">arch</a> <a href="/tags/linux/" style="font-size: 15px;">linux</a> <a href="/tags/idea/" style="font-size: 15px;">idea</a> <a href="/tags/%E6%8A%80%E6%9C%AF/" style="font-size: 15px;">技术</a> <a href="/tags/%E7%B3%BB%E7%BB%9F%E8%B0%83%E7%A0%94/" style="font-size: 15px;">系统调研</a> <a href="/tags/postgres/" style="font-size: 15px;">postgres</a> <a href="/tags/%E5%BC%80%E6%BA%90/" style="font-size: 15px;">开源</a> <a href="/tags/ChatGPT/" style="font-size: 15px;">ChatGPT</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2023/06/30/">保姆级教程：如何本地部署私有chatglm模型及知识库扩展调试</a></li><li class="post-list-item"><a class="post-list-link" href="/2023/06/24/">LangChain+ChatGLM模型自建及评测</a></li><li class="post-list-item"><a class="post-list-link" href="/2023/04/01/">ChatGPT中文调教指南</a></li><li class="post-list-item"><a class="post-list-link" href="/2023/03/28/">The-Art-of-Asking-ChatGPT(向ChatGPT提问的艺术)</a></li><li class="post-list-item"><a class="post-list-link" href="/2023/01/31/">PG实践踩坑记录</a></li><li class="post-list-item"><a class="post-list-link" href="/2023/01/28/">关于一个单领域单轮对话任务型问答系统的调研与设计</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/12/07/">主机暴毙，PG驾崩，视图数据损坏修复过程</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/08/22/">Arch系统磁盘空间清理</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/06/29/">PG下FDW插件使用</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/06/22/">当PG遇上Mysql</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="https://solo8world.github.io/leeblog/" title="暂无" target="_blank">暂无</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2023 <a href="/." rel="nofollow">Lee Blog.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=1.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/latest/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=1.0.0" async></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/latest/jquery.fancybox.min.css"><link rel="stylesheet" type="text/css" href="/css/search.css?v=1.0.0"><script type="text/javascript" src="/js/search.js?v=1.0.0"></script><script>var search_path = 'search.xml';
if (search_path.length == 0) {
  search_path = 'search.xml';
}
var path = '/' + search_path;
searchFunc(path, 'local-search-input', 'local-search-result');
</script><script type="text/javascript" src="/js/copycode.js?v=1.0.0" successtext="复制成功!"></script><link rel="stylesheet" type="text/css" href="/css/copycode.css?v=1.0.0"><script type="text/javascript" src="/js/codeblock-resizer.js?v=1.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=1.0.0"></script></div></body></html>